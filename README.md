# 決定木概要

## 1. 決定木とは

*   入力特徴量から分岐点（ノード）を作り、最終的に分類・回帰結果を得る木構造
モデル
*   分岐は「条件式」によって決定される
*   可視化が容易 → 解釈性が高い

## 2. 基本的な構造

*   **根ノード（Root）** → データ全体
*   **内部ノード（Internal node）** → 分割条件
*   **葉ノード（Leaf）** → 出力（クラスまたは数値）

例：簡易的な二項分割

```
    Root
    /   \
feature &gt; 5  feature &lt;= 5
/          \
LeafA       LeafB
```

## 3. 分割基準 (情報ゲイン / Gini係数)

*   **分類木の場合**
    *   **情報ゲイン**：エントロピー減少量
    *   **Gini係数**：不純度減少量
*   **回帰木の場合**
    *   平均二乗誤差（MSE）の減少量で選択

計算例（情報ゲイン）

```
    Gini_parent = 1 - Σ(p_i)^2
    Gini_left   = 1 - Σ(p_i_left)^2
    Gini_right  = 1 - Σ(p_i_right)^2
    ΔGini = Gini_parent - (n_left/n_parent)*Gini_left - 
(n_right/n_parent)*Gini_right
```

## 4. 剪定 (Pruning)

*   過学習を防ぐために不要な枝を削除
*   主に 2 種類
    *   **前剪定（Pre‑pruning）**：分割時に条件を満たさないと枝を作らない
    *   **後剪定（Post‑pruning）**：全木を作成後、枝を削除
*   代表的手法：Cost‑Complexity Pruning（CART）

## 5. 応用とメリット／デメリット

*   **応用例**
    *   金融リスク評価
    *   医療診断（症状 → 診断）
    *   マーケティング（顧客行動予測）
*   **メリット**
    *   解釈しやすい構造
    *   数値とカテゴリ両方の入力可
    *   前処理が少なくて済む
*   **デメリット**
    *   過学習しやすい
    *   高次元データでは枝が膨大に増える
    *   連続値の切り分けはヒューリスティック
